{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object_detection_and_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnq4IS80f3FxQX6ftRppnR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishc9/dle-notebooks/blob/master/object_detection_and_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ox0m86FdTN",
        "colab_type": "text"
      },
      "source": [
        "# Object detection\n",
        "\n",
        "## Single Shot Detector\n",
        "\n",
        "Мы разберем реализацию SSD на [tensorflow2.0](https://github.com/calmisential/TensorFlow2.0_SSD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krBXdnQIPGbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install git+https://github.com/mishc9/TensorFlow2.0_SSD.git --user -U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rlLVCD_Gjjt",
        "colab_type": "code",
        "outputId": "0ba44048-a428-43d7-a46b-dbbb958a0ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPmuErtLG7ST",
        "colab_type": "code",
        "outputId": "361fbdf8-2e82-4d35-803b-499a23061f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "!pip install tensorflow-datasets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (19.3.0)\n",
            "Requirement already satisfied: wrapt in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.21.1)\n",
            "Requirement already satisfied: absl-py in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets) (0.9.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets) (1.18.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (4.28.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets) (3.11.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /tensorflow-2.1.0/python3.6 (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.51.0)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from protobuf>=3.6.1->tensorflow-datasets) (45.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow-datasets) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow-datasets) (1.25.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow-datasets) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC_X8BOsHBvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44RBjcZggIqa",
        "colab_type": "text"
      },
      "source": [
        "## Определим loss для SSD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbYV1vdlutSm",
        "colab_type": "text"
      },
      "source": [
        "### Sigmoid focal loss\n",
        "\n",
        "Обычная кроссэнтропия: $$H(p, q) = -\\sum_i^n p_i \\cdot log(q_i)$$\n",
        "\n",
        "В случае двух классов (BCE): \n",
        "$$L = - \\big[p_i \\cdot log(\\hat{p_i}) + (1 - p_i) \\cdot log(1 - \\hat{p_i}) \\big]$$\n",
        "\n",
        "Определим $p_t$ как $\\hat{p}$, если p = 1, иначе $1 - \\hat{p}$. \n",
        "\n",
        "Тогда\n",
        " $$L = -\\log{p_t}$$\n",
        "\n",
        "\n",
        "Мы можем использовать взвешенную BCE:\n",
        "\n",
        " $$L = - \\alpha_t \\cdot -\\log{p_t}$$\n",
        "\n",
        " $\\alpha_t$ равно $\\alpha$, если p = 1, иначе $1 - \\alpha$; $\\alpha, \\gamma$ - гиперпараметры лосса. Значение $0 \\leq \\alpha \\leq 1$ - это обратная частота класса / либо подбирается вручную.\n",
        "\n",
        " Теперь, наконец, можно определить фокальный лосс:\n",
        "\n",
        "$$L = - (1 - p_t)^{\\gamma} \\cdot \\log{p_t} $$ \n",
        "\n",
        "Когда $\\gamma > 0$, плохо классифицируемые лейблы штрафуются сильнее.\n",
        "\n",
        "Совмещение этих двух подходов дает нам итоговый лосс:\n",
        "\n",
        "$$L = - \\alpha_t \\cdot (1 - p_t)^{\\gamma} \\cdot \\log{p_t} $$\n",
        "\n",
        "  подбирается, чтобы сбалансировать положительные и отрицательные классы.\n",
        "\n",
        "### Зачем это нужно?\n",
        "При детекции объектов негативных примеров >> позитивных (т.к. чаще объекта нет в кадре). Параметр $\\alpha_t$ \"выделяет\" положительные примеры.\n",
        "\n",
        "Параметр $\\gamma$ снижает штраф за \"хорошо\" классифицированные примеры.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfERWnsP003W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "alpha = (1 - 1/1000)\n",
        "gamma = 0.5\n",
        "p_t_neg = 0.99\n",
        "neg_count = 10000\n",
        "p_t_pos = 0.5\n",
        "pos_count = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVIzXZrm7F0N",
        "colab_type": "text"
      },
      "source": [
        "**У обычной BCE (Binary Cross Entropy) значительный перекос в сторону отрицательных классов**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RVqtUQ00vB",
        "colab_type": "code",
        "outputId": "08b95e9b-496a-40b3-8606-746af93f9712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "vanilla_neg_loss = -np.log(p_t_neg) * neg_count\n",
        "vanilla_pos_loss = -np.log(p_t_pos) * pos_count\n",
        "vanilla_neg_loss, vanilla_pos_loss"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100.5033585350145, 6.931471805599453)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhOpcqK27Qpn",
        "colab_type": "text"
      },
      "source": [
        "**BCE с фокальным лоссом значительно меньше штрафует хорошо различимый класс**\n",
        "\n",
        "(т.е. класс с высокой $\\hat{p}$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKR4sgOs6vfM",
        "colab_type": "code",
        "outputId": "f67ebc63-8671-4e77-86f0-e0b337625539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "focal_neg_loss = - (1 - p_t_neg)**gamma * np.log(p_t_neg) * neg_count\n",
        "focal_pos_loss = - (1 - p_t_pos)**gamma * np.log(p_t_pos) * pos_count\n",
        "focal_neg_loss, focal_pos_loss"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10.050335853501455, 4.901290717342736)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL4Uhs8y7iSI",
        "colab_type": "text"
      },
      "source": [
        "**Веса позволяют уменьшить дисбаланс классов**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrclpbIx00dB",
        "colab_type": "code",
        "outputId": "defe353a-9f16-43ae-8782-b80c7a802bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "w_focal_neg_loss = -(1 - alpha) * (1 - p_t_neg)**gamma * np.log(p_t_neg) * neg_count\n",
        "w_focal_pos_loss = -(alpha) * (1 - p_t_pos)**gamma * np.log(p_t_pos) * pos_count\n",
        "w_focal_neg_loss, w_focal_pos_loss"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.010050335853501466, 4.896389426625393)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYIVvLQ7zM0",
        "colab_type": "text"
      },
      "source": [
        "**конечно, \"взвешивание\" классов работает и без focal loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGN4PUnH7oDM",
        "colab_type": "code",
        "outputId": "4e4567b9-98c0-4708-858f-e6cf158d765e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "w_vanilla_neg_loss = (1 - alpha) * -np.log(p_t_neg) * neg_count\n",
        "w_vanilla_pos_loss = (alpha) * -np.log(p_t_pos) * pos_count\n",
        "w_vanilla_neg_loss, w_vanilla_pos_loss"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1005033585350146, 6.924540333793853)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBqWAf8SutDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_focal_loss(y_true, y_pred, alpha, gamma):\n",
        "    ce = tf.keras.backend.binary_crossentropy(target=y_true, output=y_pred, from_logits=True)\n",
        "    pred_prob = tf.math.sigmoid(y_pred)\n",
        "    p_t = (y_true * pred_prob) + ((1 - y_true) * (1 - pred_prob))\n",
        "    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "    modulating_factor = tf.math.pow((1.0 - p_t), gamma)\n",
        "    return tf.math.reduce_sum(alpha_factor * modulating_factor * ce, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRnP_Saa772H",
        "colab_type": "text"
      },
      "source": [
        "### SSD loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7R5EOLyFwnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSDLoss(object):\n",
        "    def __init__(self):\n",
        "        self.smooth_l1_loss = SmoothL1Loss()\n",
        "        self.reg_loss_weight = reg_loss_weight\n",
        "        self.cls_loss_weight = 1 - reg_loss_weight\n",
        "        self.num_classes = NUM_CLASSES + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def __cover_background_boxes(true_boxes):\n",
        "        symbol = true_boxes[..., -1]\n",
        "        mask_symbol = tf.where(symbol < 0.5, 0.0, 1.0)\n",
        "        mask_symbol = tf.expand_dims(input=mask_symbol, axis=-1)\n",
        "        cover_boxes_tensor = tf.tile(input=mask_symbol, multiples=tf.constant([1, 1, 4], dtype=tf.dtypes.int32))\n",
        "        return cover_boxes_tensor\n",
        "\n",
        "    def __call__(self, y_true, y_pred, *args, **kwargs):\n",
        "        # y_true : tensor, shape: (batch_size, total_num_of_default_boxes, 5)\n",
        "        # y_pred : tensor, shape: (batch_size, total_num_of_default_boxes, NUM_CLASSES + 5)\n",
        "        true_class = tf.cast(x=y_true[..., -1], dtype=tf.dtypes.int32)\n",
        "        pred_class = y_pred[..., :self.num_classes]\n",
        "        true_class = tf.one_hot(indices=true_class, depth=self.num_classes, axis=-1)\n",
        "        class_loss_value = tf.math.reduce_sum(sigmoid_focal_loss(y_true=true_class, y_pred=pred_class, alpha=alpha, gamma=gamma))\n",
        "\n",
        "        cover_boxes = self.__cover_background_boxes(true_boxes=y_true)\n",
        "        true_coord = y_true[..., :4] * cover_boxes\n",
        "        pred_coord = y_pred[..., self.num_classes:] * cover_boxes\n",
        "        reg_loss_value = self.smooth_l1_loss(y_true=true_coord, y_pred=pred_coord)\n",
        "\n",
        "        loss = self.cls_loss_weight * class_loss_value + self.reg_loss_weight * reg_loss_value\n",
        "        return loss, class_loss_value, reg_loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pnVfaPDo29y",
        "colab_type": "text"
      },
      "source": [
        "### SSD Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSBXbKvJo-gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSD(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(SSD, self).__init__()\n",
        "        self.num_classes = NUM_CLASSES + 1\n",
        "        self.anchor_ratios = ASPECT_RATIOS\n",
        "\n",
        "        self.backbone = ResNet50()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=1024, kernel_size=(1, 1), strides=1, padding=\"same\")\n",
        "        self.conv2_1 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1), strides=1, padding=\"same\")\n",
        "        self.conv2_2 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), strides=2, padding=\"same\")\n",
        "        self.conv3_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 1), strides=1, padding=\"same\")\n",
        "        self.conv3_2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=2, padding=\"same\")\n",
        "        self.conv4_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 1), strides=1, padding=\"same\")\n",
        "        self.conv4_2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=2, padding=\"same\")\n",
        "        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "        self.predict_1 = self._predict_layer(k=self._get_k(i=0))\n",
        "        self.predict_2 = self._predict_layer(k=self._get_k(i=1))\n",
        "        self.predict_3 = self._predict_layer(k=self._get_k(i=2))\n",
        "        self.predict_4 = self._predict_layer(k=self._get_k(i=3))\n",
        "        self.predict_5 = self._predict_layer(k=self._get_k(i=4))\n",
        "        self.predict_6 = self._predict_layer(k=self._get_k(i=5))\n",
        "\n",
        "    def _predict_layer(self, k):\n",
        "        filter_num = k * (self.num_classes + 4)\n",
        "        return tf.keras.layers.Conv2D(filters=filter_num, kernel_size=(3, 3), strides=1, padding=\"same\")\n",
        "\n",
        "    def _get_k(self, i):\n",
        "        # k is the number of boxes generated at each position of the feature map.\n",
        "        return len(self.anchor_ratios[i]) + 1\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        branch_1, x = self.backbone(inputs, training=training)\n",
        "        predict_1 = self.predict_1(branch_1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        branch_2 = x\n",
        "        predict_2 = self.predict_2(branch_2)\n",
        "\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.conv2_2(x)\n",
        "        branch_3 = x\n",
        "        predict_3 = self.predict_3(branch_3)\n",
        "\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.conv3_2(x)\n",
        "        branch_4 = x\n",
        "        predict_4 = self.predict_4(branch_4)\n",
        "\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.conv4_2(x)\n",
        "        branch_5 = x\n",
        "        predict_5 = self.predict_5(branch_5)\n",
        "\n",
        "        branch_6 = self.pool(x)\n",
        "        branch_6 = tf.expand_dims(input=branch_6, axis=1)\n",
        "        branch_6 = tf.expand_dims(input=branch_6, axis=2)\n",
        "        predict_6 = self.predict_6(branch_6)\n",
        "\n",
        "        # predict_i shape : (batch_size, h, w, k * (c+4)), where c is self.num_classes.\n",
        "        return [predict_1, predict_2, predict_3, predict_4, predict_5, predict_6]\n",
        "\n",
        "\n",
        "def ssd_prediction(feature_maps, num_classes):\n",
        "    batch_size = feature_maps[0].shape[0]\n",
        "    predicted_features_list = []\n",
        "    for feature in feature_maps:\n",
        "        predicted_features_list.append(tf.reshape(tensor=feature, shape=(batch_size, -1, num_classes + 4)))\n",
        "    predicted_features = tf.concat(values=predicted_features_list, axis=1)\n",
        "    return predicted_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzjUdYDXGyH3",
        "colab_type": "text"
      },
      "source": [
        "## Training porcess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcenM08DqPo4",
        "colab_type": "text"
      },
      "source": [
        "#### Download PASCAL Voc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTugu9pehp8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "import os\n",
        "\n",
        "gdrive_path = '/content/gdrive'\n",
        "drive.mount(gdrive_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7qHDwAUiZzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O /content/gdrive/My\\ Drive/VOCtrainval_11-May-2012.tar http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmm7-ZvskoRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf /content/gdrive/My\\ Drive/VOCtrainval_11-May-2012.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAWUuw-NlDGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjLi3R0tu6pX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsJgG77LvT_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv VOCdevkit/ dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R50f6s9PmPgj",
        "colab_type": "code",
        "outputId": "84d306ee-2782-4c3f-e7d6-bd988a9acf1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "!tree dataset -d 2"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset\n",
            "└── VOCdevkit\n",
            "    └── VOC2012\n",
            "        ├── Annotations\n",
            "        ├── ImageSets\n",
            "        │   ├── Action\n",
            "        │   ├── Layout\n",
            "        │   ├── Main\n",
            "        │   └── Segmentation\n",
            "        ├── JPEGImages\n",
            "        ├── SegmentationClass\n",
            "        └── SegmentationObject\n",
            "2 [error opening dir]\n",
            "\n",
            "11 directories\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItV_PVJQfIsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf2ssd\n",
        "from tf2ssd.core import SSD, SSDLoss, TFDataset\n",
        "import time\n",
        "from tf2ssd.core import ReadDataset, MakeGT\n",
        "from tf2ssd.core import SSD, ssd_prediction\n",
        "from tf2ssd.core import SSDLoss\n",
        "from tf2ssd.core import TFDataset\n",
        "from tf2ssd.core.inference import visualize_training_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQjvs1wRgoP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf2ssd.core.parse_voc import ParsePascalVOC\n",
        "from tf2ssd.configuration import TXT_DIR\n",
        "\n",
        "voc = ParsePascalVOC()\n",
        "voc.write_data_to_txt(txt_dir=TXT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5XLuFge-Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU settings\n",
        "from tf2ssd.configuration import IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS, EPOCHS, NUM_CLASSES, BATCH_SIZE, save_model_dir, \\\n",
        "    load_weights_before_training, load_weights_from_epoch, save_frequency, test_images_during_training, \\\n",
        "    test_images_dir_list\n",
        "\n",
        "def print_model_summary(network):\n",
        "    network.build(input_shape=(None, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n",
        "    network.summary()\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "dataset = TFDataset()\n",
        "train_data, train_count = dataset.generate_datatset()\n",
        "\n",
        "ssd = SSD()\n",
        "print_model_summary(network=ssd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NwXbq53fBPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if load_weights_before_training:\n",
        "    ssd.load_weights(filepath=save_model_dir+\"epoch-{}\".format(load_weights_from_epoch))\n",
        "    print(\"Successfully load weights!\")\n",
        "else:\n",
        "    load_weights_from_epoch = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD--e9IooaU5",
        "colab_type": "text"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNzxhrbXp1QC",
        "colab_type": "text"
      },
      "source": [
        "#### Define training step first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLfe03qro0AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss\n",
        "loss = SSDLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# metrics\n",
        "loss_metric = tf.metrics.Mean()\n",
        "cls_loss_metric = tf.metrics.Mean()\n",
        "reg_loss_metric = tf.metrics.Mean()\n",
        "\n",
        "\n",
        "def train_step(batch_images, batch_labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = ssd(batch_images, training=True)\n",
        "        output = ssd_prediction(feature_maps=pred, num_classes=NUM_CLASSES + 1)\n",
        "        gt = MakeGT(batch_labels, pred)\n",
        "        gt_boxes = gt.generate_gt_boxes()\n",
        "        loss_value, cls_loss, reg_loss = loss(y_true=gt_boxes, y_pred=output)\n",
        "    gradients = tape.gradient(loss_value, ssd.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(gradients, ssd.trainable_variables))\n",
        "    loss_metric.update_state(values=loss_value)\n",
        "    cls_loss_metric.update_state(values=cls_loss)\n",
        "    reg_loss_metric.update_state(values=reg_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "771basNxp9Nh",
        "colab_type": "text"
      },
      "source": [
        "#### Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2NkIAOeGx4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "3345ae36-2073-4b92-86f3-549180ba3f5b"
      },
      "source": [
        "for epoch in range(load_weights_from_epoch + 1, EPOCHS):\n",
        "    for step, batch_data in enumerate(train_data):\n",
        "        start_time = time.time()\n",
        "        images, labels = ReadDataset().read(batch_data)\n",
        "        train_step(batch_images=images, batch_labels=labels)\n",
        "        spent_time = time.time() - start_time\n",
        "        print(\"Epoch: {}/{}, step: {}/{}, time spent: {:.2f}s, loss: {:.5f}, \"\n",
        "              \"cls loss: {:.5f}, reg loss: {:.5f}\".format(epoch,\n",
        "                                                          EPOCHS,\n",
        "                                                          step,\n",
        "                                                          tf.math.ceil(train_count / BATCH_SIZE),\n",
        "                                                          spent_time,\n",
        "                                                          loss_metric.result(),\n",
        "                                                          cls_loss_metric.result(),\n",
        "                                                          reg_loss_metric.result()))\n",
        "    loss_metric.reset_states()\n",
        "    cls_loss_metric.reset_states()\n",
        "    reg_loss_metric.reset_states()\n",
        "\n",
        "    if epoch % save_frequency == 0:\n",
        "        ssd.save_weights(filepath=save_model_dir+\"epoch-{}\".format(epoch), save_format=\"tf\")\n",
        "\n",
        "    if test_images_during_training:\n",
        "        visualize_training_results(pictures=test_images_dir_list, model=ssd, epoch=epoch)\n",
        "\n",
        "ssd.save_weights(filepath=save_model_dir+\"saved_model\", save_format=\"tf\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/50, step: 665/2141.0, time spent: 4.18s, loss: 3099724.00000, cls loss: 5195248.00000, reg loss: 1004202.50000\n",
            "Epoch: 0/50, step: 666/2141.0, time spent: 4.29s, loss: 3095166.00000, cls loss: 5187529.50000, reg loss: 1002804.68750\n",
            "Epoch: 0/50, step: 667/2141.0, time spent: 4.23s, loss: 3090773.00000, cls loss: 5179988.50000, reg loss: 1001559.68750\n",
            "Epoch: 0/50, step: 668/2141.0, time spent: 3.65s, loss: 3086153.75000, cls loss: 5172247.00000, reg loss: 1000062.56250\n",
            "Epoch: 0/50, step: 669/2141.0, time spent: 5.25s, loss: 3081571.00000, cls loss: 5164538.50000, reg loss: 998605.43750\n",
            "Epoch: 0/50, step: 670/2141.0, time spent: 3.91s, loss: 3077556.00000, cls loss: 5157254.00000, reg loss: 997859.87500\n",
            "Epoch: 0/50, step: 671/2141.0, time spent: 3.94s, loss: 3073425.25000, cls loss: 5150091.00000, reg loss: 996761.62500\n",
            "Epoch: 0/50, step: 672/2141.0, time spent: 3.81s, loss: 3068875.25000, cls loss: 5142459.50000, reg loss: 995292.81250\n",
            "Epoch: 0/50, step: 673/2141.0, time spent: 3.24s, loss: 3064368.00000, cls loss: 5134909.00000, reg loss: 993829.43750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MXy-rAQKWQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X43aYDimgDxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}